{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import nltk\n",
    "import statistics\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMs TO TWEAK\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "NUM_BUCKET = 10\n",
    "SPLIT_PERCENTAGE = 0.8\n",
    "GAMMA = 1\n",
    "NGRAM = 4\n",
    "OTHER_USERS = ['Thehealeroftri.data', 'User_Name13.data', 'maxwellhill.data', \n",
    "               'illuminatedwax.data', 'Rlight.data', 'straydog1980.data', 'themightiestduck.data', \n",
    "               'qgyh2.data', 'BritishEnglishPolice.data', 'IAmAN00bie.data', 'manbra.data', \n",
    "               'MaiaNyx.data', 'nix0n.data', 'Jux_.data', '_vargas_.data', '-eDgaR-.data', \n",
    "               'Abe_lincolin.data', 'Ambiguously_Ironic.data', 'anutensil.data', 'APOSTOLATE.data',\n",
    "            'awildsketchappeared.data', 'axolotl_peyotl.data','boib.data']\n",
    "NUM_OTHERS = len(OTHER_USERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994, 294)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_comments(comments):\n",
    "    total_comments = len(comments)\n",
    "    split1 = comments[:int(total_comments * SPLIT_PERCENTAGE)]\n",
    "    split2 = comments[int(total_comments * SPLIT_PERCENTAGE):]\n",
    "    return (split1, split2)\n",
    "\n",
    "sahil = None\n",
    "with open('./data/vrckid.data', 'r') as f:\n",
    "    sahil = f.read()\n",
    "    sahil = json.loads(sahil)\n",
    "    \n",
    "# DECIDE TRANINING SAMPLE FOR SAHIL\n",
    "sahil_split1, sahil_split2 = split_comments(sahil[:700])\n",
    "len(sahil), len(sahil[700:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data of other sample users\n",
    "others = []\n",
    "for user in OTHER_USERS:\n",
    "    with open('./data/' + user, 'r') as f:\n",
    "        other = f.read()\n",
    "        other = json.loads(other)\n",
    "        others.append(other)\n",
    "len(others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_ngrams(comments):\n",
    "    # returns the set of ngrams given \n",
    "    all_text = \" \".join(comments)\n",
    "    words = word_tokenizer.tokenize(all_text.lower())\n",
    "    n_grams = set(nltk.ngrams(\" \".join(words), NGRAM))\n",
    "    return n_grams\n",
    "\n",
    "def parition_ngram(comments = sahil_split2):\n",
    "    # returns ngrams of buckets of text\n",
    "    all_text = \" \".join(comments)\n",
    "    words = word_tokenizer.tokenize(all_text.lower())\n",
    "    buckets = [words[i::NUM_BUCKET] for i in xrange(NUM_BUCKET)]\n",
    "    n_gram_buckets = [get_all_ngrams(bucket) for bucket in buckets]\n",
    "    return n_gram_buckets\n",
    "\n",
    "def get_ngram_for_user_block(block):\n",
    "    # Returns ngram given a user block\n",
    "    assert  block < NUM_BUCKET\n",
    "    return parition_ngram()[block]\n",
    "\n",
    "def calculate_ru_block(n_gram_block):\n",
    "    # calculates percentage of unique n-gram models for given block\n",
    "    num = len(set.intersection(n_gram_block, get_all_ngrams(sahil_split1)))\n",
    "    den = len(n_gram_block)\n",
    "    return num / float(den)\n",
    "\n",
    "def get_ru_for_user_block(block):\n",
    "    # calculates percentage of unique n-gram models for a block of user data\n",
    "    assert block < NUM_BUCKET\n",
    "    n_gram_block = get_ngram_for_user_block(block)\n",
    "    return calculate_ru_block(n_gram_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8790858314799795, 0.00014092618112231532, 0.011871233344615685)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_blocks_ru_values = []\n",
    "for other_comments in others:\n",
    "    other_comments1, other_comments2 = split_comments(other_comments)\n",
    "    partitions = parition_ngram(comments=other_comments2)\n",
    "    ru_values = [calculate_ru_block(n_gram_block) for n_gram_block in partitions]\n",
    "    other_blocks_ru_values.append(ru_values)\n",
    "\n",
    "ru_values_of_all_blocks = [get_ru_for_user_block(block) for block in xrange(NUM_BUCKET)]\n",
    "sample_mean = statistics.mean(ru_values_of_all_blocks)\n",
    "sample_variance = statistics.variance(ru_values_of_all_blocks)\n",
    "sample_std_dev = statistics.stdev(ru_values_of_all_blocks)\n",
    "sample_mean, sample_variance, sample_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(threshold):\n",
    "    FA, FR = 0.0, 0.0\n",
    "    threshold_gamma_sum = threshold + GAMMA\n",
    "    for i in xrange(NUM_BUCKET):\n",
    "        if ru_values_of_all_blocks[i] < threshold_gamma_sum:\n",
    "            FR += 1\n",
    "    FRR = FR / float(NUM_BUCKET)\n",
    "    for k in xrange(NUM_OTHERS):       \n",
    "        for j in xrange(NUM_BUCKET):\n",
    "            if other_blocks_ru_values[k][j] >= threshold_gamma_sum:\n",
    "                FA += 1\n",
    "    FAR = FA / float(NUM_BUCKET * NUM_OTHERS)\n",
    "    if FAR == FRR:\n",
    "        FRR += 0.00001\n",
    "    return (FAR, FRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_user_threshold():\n",
    "    up = False\n",
    "    down = False\n",
    "    delta = 1\n",
    "    threshold = sample_mean - (sample_std_dev / 2.0)\n",
    "    while delta > 0.0001:\n",
    "        FAR, FRR = calculate(threshold)\n",
    "        if FRR - FAR > 0:\n",
    "            down = True\n",
    "            threshold -= delta\n",
    "        if FAR - FRR > 0:\n",
    "            up = True\n",
    "            threshold += delta\n",
    "        if up and down:\n",
    "            up = False\n",
    "            down = False\n",
    "            delta = delta / 10.0\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14184978519232827"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = train_for_user_threshold()\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_comment(comment):\n",
    "    ngrams = get_all_ngrams([comment])\n",
    "    if not ngrams:\n",
    "        return 0\n",
    "    return 1 if calculate_ru_block(ngrams) > (threshold + GAMMA) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.659932659933\n"
     ]
    }
   ],
   "source": [
    "num_sahil_test_comments = len (sahil[700:])\n",
    "test_others = []\n",
    "test_user_list = [\n",
    " 'Dacvak.data',\n",
    " 'X019.data',\n",
    " 'Slouching2Bethlehem.data',\n",
    " 'DaedalusMinion.data',\n",
    " 'stopscopiesme.data',\n",
    " 'girafa.data']\n",
    "\n",
    "for user in test_user_list:\n",
    "    with open('./data/' + user, 'r') as f:\n",
    "        other = f.read()\n",
    "        other = json.loads(other)\n",
    "        shuffle(other)\n",
    "        test_others.extend(other[:50])\n",
    "\n",
    "expected_val = [0] * len(test_others) + [1] * num_sahil_test_comments\n",
    "        \n",
    "test_others = test_others + sahil[700:]\n",
    "res = [classify_comment(comment) for comment in test_others]\n",
    "\n",
    "correct_count = 0\n",
    "for predicted, expected in zip(res, expected_val):\n",
    "    if predicted == expected:\n",
    "        correct_count += 1\n",
    "print 'Accuracy=', correct_count /float(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
