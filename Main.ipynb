{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from sklearn import linear_model\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AuthorshipMasterClassifier(object):\n",
    "\n",
    "    def __init__(self, author_path, rando_path):\n",
    "        with open(author_path, mode='r') as f:\n",
    "            text = f.read()\n",
    "            self.author_text = json.loads(text)\n",
    "        shuffle(self.author_text)\n",
    "        #randomly get samples\n",
    "        self.rando_text = []\n",
    "        #file_list = os.listdir(rando_path)\n",
    "        file_list = ['_vargas_.data', '-eDgaR-.data', 'Abe_lincolin.data',\n",
    "                     'Ambiguously_Ironic.data', 'anutensil.data', 'APOSTOLATE.data',\n",
    "                    'awildsketchappeared.data', 'axolotl_peyotl.data','boib.data']\n",
    "        \"\"\"\n",
    "        'DaedalusMinion.data',\n",
    "                    'Donald_Keyman.data', 'Elaus.data', 'dick-nipples.data', 'IAmTheRedWizards.data'\n",
    "        \"\"\"\n",
    "        shuffle(file_list)\n",
    "        for filename in file_list:\n",
    "            real_path = rando_path + filename\n",
    "            if filename.startswith('.') or real_path == author_path:\n",
    "                continue\n",
    "            with open(real_path, mode='r') as f:\n",
    "                text = f.read()\n",
    "                text_json = json.loads(text)\n",
    "                shuffle(text_json)\n",
    "                self.rando_text.extend(text_json[:len(self.author_text)/20])\n",
    "            if len(self.rando_text) >= len(self.author_text):\n",
    "                break\n",
    "        shuffle(self.rando_text)\n",
    "        self.subclassifier_list = []\n",
    "        self._train_test_split(0.7)\n",
    "\n",
    "    def _train_test_split(self, split):\n",
    "        self.train_samples = [ (comment, 1) for comment in self.author_text[:int(len(self.author_text) * split)]]\n",
    "        self.train_samples.extend([(comment, 0) for comment in self.rando_text[:int(len(self.rando_text) * split)]])\n",
    "        self.test_samples = [ (comment, 1) for comment in self.author_text[int(len(self.author_text) * split):]]\n",
    "        self.test_samples.extend([(comment, 0) for comment in self.rando_text[int(len(self.rando_text) * split):]])\n",
    "        shuffle(self.train_samples)\n",
    "        shuffle(self.test_samples)\n",
    "\n",
    "    def add_subclassifier(self, slave):\n",
    "        slave.initialize(self.author_text, self.rando_text)\n",
    "        slave.build_model(self.train_samples)\n",
    "        self.subclassifier_list.append(slave)\n",
    "        return self\n",
    "\n",
    "    def predict(self, comment):\n",
    "        one_counts = 0\n",
    "        zero_counts = 0\n",
    "        for slave in self.subclassifier_list:\n",
    "            result = slave.predict(comment)\n",
    "            if result == 1:\n",
    "                one_counts += 1\n",
    "            elif result == 0:\n",
    "                zero_counts += 1\n",
    "            else:\n",
    "                raise Exception(\"Sad\")\n",
    "        return int(one_counts > zero_counts)\n",
    "    \n",
    "    def test(self):\n",
    "        correct = 0.0\n",
    "        for comment, expected in self.test_samples:\n",
    "            predicted = self.predict(comment)\n",
    "            if predicted == expected:\n",
    "                correct += 1\n",
    "        return correct / len(self.test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BagOfWordsClassifier(object):\n",
    "    def __init__(self, top_most=500):\n",
    "        self.tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "        self.logreg = linear_model.LogisticRegression()\n",
    "        self.top_most = top_most\n",
    "    def initialize(self, author_text, rando_text):\n",
    "        author_bag = Counter()\n",
    "        for comment in author_text:\n",
    "            for word in self.tokenizer.tokenize(comment):\n",
    "                author_bag[word] += 1\n",
    "        self.feature_set = set(word for word, count in author_bag.most_common(self.top_most))\n",
    "\n",
    "    def build_feature_row(self, comment):\n",
    "        row = [0] * (len(self.feature_set))\n",
    "        comment_set = set(self.tokenizer.tokenize(comment))\n",
    "        for i, word in enumerate(self.feature_set):\n",
    "            if word in comment_set:\n",
    "                row[i] = 1\n",
    "        return row\n",
    "        \n",
    "    def build_model(self, train_samples):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for comment, expected in train_samples:\n",
    "            X.append(self.build_feature_row(comment))\n",
    "            Y.append(expected)\n",
    "        self.logreg.fit(X, Y)\n",
    "    def predict(self, comment):\n",
    "        X = [self.build_feature_row(comment)]\n",
    "        return self.logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterNGramsClassifier(object):\n",
    "    def __init__(self, n=2, top_most=500):\n",
    "        self.n = n\n",
    "        self.top_most = top_most\n",
    "        self.logreg = linear_model.LogisticRegression()\n",
    "\n",
    "    def initialize(self, author_text, rando_text):\n",
    "        author_bag = Counter()\n",
    "        for comment in author_text:\n",
    "            for gram in nltk.ngrams(comment, self.n):\n",
    "                author_bag[gram] += 1\n",
    "        self.feature_set = {gram for gram, freq in author_bag.most_common(self.top_most)}\n",
    "\n",
    "    def build_feature_row(self, comment):\n",
    "        row = [0] * (len(self.feature_set))\n",
    "        ngram_set = set(nltk.ngrams(comment, self.n))\n",
    "        for i, gram in enumerate(self.feature_set):\n",
    "            if gram in ngram_set:\n",
    "                row[i] = 1\n",
    "        return row\n",
    "\n",
    "    def build_model(self, train_samples):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for comment, expected in train_samples:\n",
    "            X.append(self.build_feature_row(comment))\n",
    "            Y.append(expected)\n",
    "        self.logreg.fit(X, Y)\n",
    "\n",
    "    def predict(self, comment):\n",
    "        X = [self.build_feature_row(comment)]\n",
    "        return self.logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PartOfSpeechClassifier(object):\n",
    "    def __init__(self, n=2, top_most=400):\n",
    "        self.tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "        self.n = n\n",
    "        self.top_most = top_most\n",
    "        self.logreg = linear_model.LogisticRegression()\n",
    "\n",
    "    def initialize(self, author_text, rando_text):\n",
    "        author_pos = []\n",
    "        for comment in author_text:\n",
    "            tokens = self.tokenizer.tokenize(comment)\n",
    "            pos = nltk.pos_tag(tokens)\n",
    "            author_pos.append([tag for w, tag in pos])\n",
    "        author_counter = Counter()\n",
    "        for pos in author_pos:\n",
    "            for gram in nltk.ngrams(pos, self.n):\n",
    "                author_counter[gram] += 1\n",
    "        self.feature_set = {pos for pos, freq in author_counter.most_common(self.top_most)}\n",
    "\n",
    "    def build_feature_row(self, comment):\n",
    "        row = [0] * (len(self.feature_set))\n",
    "        tokens = self.tokenizer.tokenize(comment)\n",
    "        pos = nltk.pos_tag(tokens)\n",
    "        ngram_set = set(nltk.ngrams(pos, self.n))\n",
    "        for i, gram in enumerate(self.feature_set):\n",
    "            if gram in ngram_set:\n",
    "                row[i] = 1\n",
    "        return row\n",
    "\n",
    "    def build_model(self, train_samples):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for comment, expected in train_samples:\n",
    "            X.append(self.build_feature_row(comment))\n",
    "            Y.append(expected)\n",
    "        self.logreg.fit(X, Y)\n",
    "\n",
    "    def predict(self, comment):\n",
    "        X = [self.build_feature_row(comment)]\n",
    "        return self.logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  1  score= 0.756944444444\n",
      "Test  2  score= 0.75462962963\n",
      "Test  3  score= 0.747685185185\n",
      "Test  4  score= 0.759259259259\n",
      "Test  5  score= 0.74537037037\n",
      "Test  6  score= 0.766203703704\n",
      "Test  7  score= 0.74537037037\n",
      "Test  8  score= 0.738425925926\n",
      "Test  9  score= 0.766203703704\n",
      "Test  10  score= 0.75\n",
      "Average score =  0.753009259259\n"
     ]
    }
   ],
   "source": [
    "test_times = 10\n",
    "total = 0.0\n",
    "for i in xrange(test_times):\n",
    "    cl = AuthorshipMasterClassifier('./data/vrckid.data', './data/')\n",
    "    cl.add_subclassifier(CharacterNGramsClassifier())\n",
    "    cl.add_subclassifier(BagOfWordsClassifier())\n",
    "    cl.add_subclassifier(PartOfSpeechClassifier(n=2))\n",
    "    score = cl.test()\n",
    "    print 'Test ', i + 1, ' score=', score\n",
    "    total += score\n",
    "print 'Average score = ', total / test_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
