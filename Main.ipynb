{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/himankyadav/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from sklearn import linear_model\n",
    "import nltk\n",
    "import os\n",
    "import statistics\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorshipMasterClassifier(object):\n",
    "\n",
    "    def __init__(self, author_path, rando_path):\n",
    "        with open(author_path, mode='r') as f:\n",
    "            text = f.read()\n",
    "            self.author_text = json.loads(text)\n",
    "        shuffle(self.author_text)\n",
    "        #randomly get samples\n",
    "        self.rando_text = []\n",
    "        #file_list = os.listdir(rando_path)\n",
    "        file_list = ['_vargas_.data', '-eDgaR-.data', 'Abe_lincolin.data',\n",
    "                     'Ambiguously_Ironic.data', 'anutensil.data', 'APOSTOLATE.data',\n",
    "                    'awildsketchappeared.data', 'axolotl_peyotl.data','boib.data']\n",
    "        \"\"\"\n",
    "        'DaedalusMinion.data',\n",
    "                    'Donald_Keyman.data', 'Elaus.data', 'dick-nipples.data', 'IAmTheRedWizards.data'\n",
    "        \"\"\"\n",
    "        shuffle(file_list)\n",
    "        for filename in file_list:\n",
    "            real_path = rando_path + filename\n",
    "            if filename.startswith('.') or real_path == author_path:\n",
    "                continue\n",
    "            with open(real_path, mode='r') as f:\n",
    "                text = f.read()\n",
    "                text_json = json.loads(text)\n",
    "                shuffle(text_json)\n",
    "                self.rando_text.extend(text_json[:len(self.author_text)/20])\n",
    "            if len(self.rando_text) >= len(self.author_text):\n",
    "                break\n",
    "        shuffle(self.rando_text)\n",
    "        self.subclassifier_list = []\n",
    "        self._train_test_split(0.7)\n",
    "\n",
    "    def _train_test_split(self, split):\n",
    "        self.train_samples = [ (comment, 1) for comment in self.author_text[:int(len(self.author_text) * split)]]\n",
    "        self.train_samples.extend([(comment, 0) for comment in self.rando_text[:int(len(self.rando_text) * split)]])\n",
    "        self.test_samples = [ (comment, 1) for comment in self.author_text[int(len(self.author_text) * split):]]\n",
    "        self.test_samples.extend([(comment, 0) for comment in self.rando_text[int(len(self.rando_text) * split):]])\n",
    "        shuffle(self.train_samples)\n",
    "        shuffle(self.test_samples)\n",
    "\n",
    "    def add_subclassifier(self, slave):\n",
    "        slave.initialize(self.author_text, self.rando_text)\n",
    "        slave.build_model(self.train_samples)\n",
    "        self.subclassifier_list.append(slave)\n",
    "        return self\n",
    "\n",
    "    def predict(self, comment):\n",
    "        one_counts = 0\n",
    "        zero_counts = 0\n",
    "        for slave in self.subclassifier_list:\n",
    "            result = slave.predict(comment)\n",
    "            if result == 1:\n",
    "                one_counts += 1\n",
    "            elif result == 0:\n",
    "                zero_counts += 1\n",
    "            else:\n",
    "                raise Exception(\"Sad\")\n",
    "        return int(one_counts > zero_counts)\n",
    "    \n",
    "    def test(self):\n",
    "        correct = 0.0\n",
    "        for comment, expected in self.test_samples:\n",
    "            predicted = self.predict(comment)\n",
    "            if predicted == expected:\n",
    "                correct += 1\n",
    "        return correct / len(self.test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWordsClassifier(object):\n",
    "    def __init__(self, top_most=500):\n",
    "        self.tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "        self.logreg = linear_model.LogisticRegression()\n",
    "        self.top_most = top_most\n",
    "    def initialize(self, author_text, rando_text):\n",
    "        author_bag = Counter()\n",
    "        for comment in author_text:\n",
    "            for word in self.tokenizer.tokenize(comment):\n",
    "                author_bag[word] += 1\n",
    "        self.feature_set = set(word for word, count in author_bag.most_common(self.top_most))\n",
    "\n",
    "    def build_feature_row(self, comment):\n",
    "        row = [0] * (len(self.feature_set))\n",
    "        comment_set = set(self.tokenizer.tokenize(comment))\n",
    "        for i, word in enumerate(self.feature_set):\n",
    "            if word in comment_set:\n",
    "                row[i] = 1\n",
    "        return row\n",
    "        \n",
    "    def build_model(self, train_samples):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for comment, expected in train_samples:\n",
    "            X.append(self.build_feature_row(comment))\n",
    "            Y.append(expected)\n",
    "        self.logreg.fit(X, Y)\n",
    "    def predict(self, comment):\n",
    "        X = [self.build_feature_row(comment)]\n",
    "        return self.logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterNGramsClassifier(object):\n",
    "    def __init__(self, n=2, top_most=500):\n",
    "        self.n = n\n",
    "        self.top_most = top_most\n",
    "        self.logreg = linear_model.LogisticRegression()\n",
    "\n",
    "    def initialize(self, author_text, rando_text):\n",
    "        author_bag = Counter()\n",
    "        for comment in author_text:\n",
    "            for gram in nltk.ngrams(comment, self.n):\n",
    "                author_bag[gram] += 1\n",
    "        self.feature_set = {gram for gram, freq in author_bag.most_common(self.top_most)}\n",
    "\n",
    "    def build_feature_row(self, comment):\n",
    "        row = [0] * (len(self.feature_set))\n",
    "        ngram_set = set(nltk.ngrams(comment, self.n))\n",
    "        for i, gram in enumerate(self.feature_set):\n",
    "            if gram in ngram_set:\n",
    "                row[i] = 1\n",
    "        return row\n",
    "\n",
    "    def build_model(self, train_samples):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for comment, expected in train_samples:\n",
    "            X.append(self.build_feature_row(comment))\n",
    "            Y.append(expected)\n",
    "        self.logreg.fit(X, Y)\n",
    "\n",
    "    def predict(self, comment):\n",
    "        X = [self.build_feature_row(comment)]\n",
    "        return self.logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartOfSpeechClassifier(object):\n",
    "    def __init__(self, n=2, top_most=400):\n",
    "        self.tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "        self.n = n\n",
    "        self.top_most = top_most\n",
    "        self.logreg = linear_model.LogisticRegression()\n",
    "\n",
    "    def initialize(self, author_text, rando_text):\n",
    "        author_pos = []\n",
    "        for comment in author_text:\n",
    "            tokens = self.tokenizer.tokenize(comment)\n",
    "            pos = nltk.pos_tag(tokens)\n",
    "            author_pos.append([tag for w, tag in pos])\n",
    "        author_counter = Counter()\n",
    "        for pos in author_pos:\n",
    "            for gram in nltk.ngrams(pos, self.n):\n",
    "                author_counter[gram] += 1\n",
    "        self.feature_set = {pos for pos, freq in author_counter.most_common(self.top_most)}\n",
    "\n",
    "    def build_feature_row(self, comment):\n",
    "        row = [0] * (len(self.feature_set))\n",
    "        tokens = self.tokenizer.tokenize(comment)\n",
    "        pos = nltk.pos_tag(tokens)\n",
    "        ngram_set = set(nltk.ngrams(pos, self.n))\n",
    "        for i, gram in enumerate(self.feature_set):\n",
    "            if gram in ngram_set:\n",
    "                row[i] = 1\n",
    "        return row\n",
    "\n",
    "    def build_model(self, train_samples):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for comment, expected in train_samples:\n",
    "            X.append(self.build_feature_row(comment))\n",
    "            Y.append(expected)\n",
    "        self.logreg.fit(X, Y)\n",
    "\n",
    "    def predict(self, comment):\n",
    "        X = [self.build_feature_row(comment)]\n",
    "        return self.logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMs TO TWEAK\n",
    "class ShortMessageVerificationClassifier(object):\n",
    "    def __init__(self, NUM_BUCKET = 10, SPLIT_PERCENTAGE = 0.8, GAMMA = 1, NGRAM = 4):\n",
    "        self.NUM_BUCKET = NUM_BUCKET\n",
    "        self.SPLIT_PERCENTAGE = SPLIT_PERCENTAGE\n",
    "        self.GAMMA = GAMMA\n",
    "        self.NGRAM = NGRAM\n",
    "        self.word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "        self.OTHER_USERS = ['Thehealeroftri.data', 'User_Name13.data', 'maxwellhill.data', \n",
    "                       'illuminatedwax.data', 'Rlight.data', 'straydog1980.data', 'themightiestduck.data', \n",
    "                       'qgyh2.data', 'BritishEnglishPolice.data', 'IAmAN00bie.data', 'manbra.data', \n",
    "                       'MaiaNyx.data', 'nix0n.data', 'Jux_.data', '_vargas_.data', '-eDgaR-.data', \n",
    "                       'Abe_lincolin.data', 'Ambiguously_Ironic.data', 'anutensil.data', 'APOSTOLATE.data',\n",
    "                    'awildsketchappeared.data', 'axolotl_peyotl.data','boib.data']\n",
    "        self.NUM_OTHERS = len(self.OTHER_USERS)\n",
    "        \n",
    "    def initialize(self, author_text, rando_text):\n",
    "        pass\n",
    "        \n",
    "    def split_comments(self, comments):\n",
    "        total_comments = len(comments)\n",
    "        split1 = comments[:int(total_comments * self.SPLIT_PERCENTAGE)]\n",
    "        split2 = comments[int(total_comments * self.SPLIT_PERCENTAGE):]\n",
    "        return (split1, split2)\n",
    "            \n",
    "    def build_model(self, train_samples):\n",
    "        # DECIDE TRANINING SAMPLE FOR SAHIL\n",
    "        author_train_samples = [comment for (comment, expected) in train_samples if expected]\n",
    "        self.sahil_split1, self.sahil_split2 = self.split_comments(author_train_samples)\n",
    "        # Data of other sample users\n",
    "        others = []\n",
    "        for user in self.OTHER_USERS:\n",
    "            with open('./data/' + user, 'r') as f:\n",
    "                other = f.read()\n",
    "                other = json.loads(other)\n",
    "                others.append(other)\n",
    "                \n",
    "        other_blocks_ru_values = []\n",
    "        \n",
    "        for other_comments in others:\n",
    "            other_comments1, other_comments2 = self.split_comments(other_comments)\n",
    "            partitions = self.parition_ngram(other_comments2)\n",
    "            ru_values = [self.calculate_ru_block(n_gram_block) for n_gram_block in partitions]\n",
    "            other_blocks_ru_values.append(ru_values)\n",
    "\n",
    "        ru_values_of_all_blocks = [self.get_ru_for_user_block(block) for block in xrange(self.NUM_BUCKET)]\n",
    "        sample_mean = statistics.mean(ru_values_of_all_blocks)\n",
    "        sample_variance = statistics.variance(ru_values_of_all_blocks)\n",
    "        sample_std_dev = statistics.stdev(ru_values_of_all_blocks)\n",
    "        self.threshold = self.train_for_user_threshold(sample_mean, sample_variance, ru_values_of_all_blocks, other_blocks_ru_values)\n",
    "                \n",
    "    def get_all_ngrams(self, comments):\n",
    "        # returns the set of ngrams given \n",
    "        all_text = \" \".join(comments)\n",
    "        words = self.word_tokenizer.tokenize(all_text.lower())\n",
    "        n_grams = set(nltk.ngrams(\" \".join(words), self.NGRAM))\n",
    "        return n_grams\n",
    "\n",
    "    def parition_ngram(self, comments):\n",
    "        # returns ngrams of buckets of text\n",
    "        all_text = \" \".join(comments)\n",
    "        words = self.word_tokenizer.tokenize(all_text.lower())\n",
    "        buckets = [words[i::self.NUM_BUCKET] for i in xrange(self.NUM_BUCKET)]\n",
    "        n_gram_buckets = [self.get_all_ngrams(bucket) for bucket in buckets]\n",
    "        return n_gram_buckets\n",
    "\n",
    "    def get_ngram_for_user_block(self, block):\n",
    "        # Returns ngram given a user block\n",
    "        assert  block < self.NUM_BUCKET\n",
    "        return self.parition_ngram(self.sahil_split2)[block]\n",
    "\n",
    "    def calculate_ru_block(self, n_gram_block):\n",
    "        # calculates percentage of unique n-gram models for given block\n",
    "        num = len(set.intersection(n_gram_block, self.get_all_ngrams(self.sahil_split1)))\n",
    "        den = len(n_gram_block)\n",
    "        return num / float(den)\n",
    "\n",
    "    def get_ru_for_user_block(self, block):\n",
    "        # calculates percentage of unique n-gram models for a block of user data\n",
    "        assert block < self.NUM_BUCKET\n",
    "        n_gram_block = self.get_ngram_for_user_block(block)\n",
    "        return self.calculate_ru_block(n_gram_block)\n",
    "    \n",
    "    def train_for_user_threshold(self, sample_mean, sample_std_dev, ru_values_of_all_blocks, other_blocks_ru_values):\n",
    "        up = False\n",
    "        down = False\n",
    "        delta = 1\n",
    "        threshold = sample_mean - (sample_std_dev / 2.0)\n",
    "        while delta > 0.0001:\n",
    "            FAR, FRR = self.calculate(threshold, ru_values_of_all_blocks, other_blocks_ru_values)\n",
    "            if FRR - FAR > 0:\n",
    "                down = True\n",
    "                threshold -= delta\n",
    "            if FAR - FRR > 0:\n",
    "                up = True\n",
    "                threshold += delta\n",
    "            if up and down:\n",
    "                up = False\n",
    "                down = False\n",
    "                delta = delta / 10.0\n",
    "        return threshold\n",
    "    \n",
    "    def calculate(self, threshold, ru_values_of_all_blocks, other_blocks_ru_values):\n",
    "        FA, FR = 0.0, 0.0\n",
    "        threshold_gamma_sum = threshold + self.GAMMA\n",
    "        for i in xrange(self.NUM_BUCKET):\n",
    "            if ru_values_of_all_blocks[i] < threshold_gamma_sum:\n",
    "                FR += 1\n",
    "        FRR = FR / float(self.NUM_BUCKET)\n",
    "        for k in xrange(self.NUM_OTHERS):       \n",
    "            for j in xrange(self.NUM_BUCKET):\n",
    "                if other_blocks_ru_values[k][j] >= threshold_gamma_sum:\n",
    "                    FA += 1\n",
    "        FAR = FA / float(self.NUM_BUCKET * self.NUM_OTHERS)\n",
    "        if FAR == FRR:\n",
    "            FRR += 0.00001\n",
    "        return (FAR, FRR)\n",
    "    \n",
    "    def predict(self, comment):\n",
    "        ngrams = self.get_all_ngrams([comment])\n",
    "        if not ngrams:\n",
    "            return 0\n",
    "        return 1 if self.calculate_ru_block(ngrams) > (self.threshold + self.GAMMA) else 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalKMeansClustering():\n",
    "    def __init__(self):\n",
    "        self.sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "    def initialize(self, author_text, rando_text):\n",
    "        pass\n",
    "    \n",
    "    def build_model(self, train_samples):\n",
    "        # print pacling.classify('\\n'.join(sahil), '\\n'.join(sahil))\n",
    "        comments = [comment for (comment, expected) in train_samples if expected]\n",
    "        self.comments = filter(lambda k: k != \"\", comments)\n",
    "\n",
    "        fvs_lexical = np.zeros((len(comments), 3), np.float64)\n",
    "        fvs_punct = np.zeros((len(comments), 3), np.float64)\n",
    "        for e, ch_text in enumerate(comments):\n",
    "            tokens = nltk.word_tokenize(ch_text.lower())\n",
    "            words = self.word_tokenizer.tokenize(ch_text.lower())\n",
    "            sentences = self.sentence_tokenizer.tokenize(ch_text)\n",
    "            vocab = set(words)\n",
    "            words_per_sentence = np.array([len(self.word_tokenizer.tokenize(s)) for s in sentences])\n",
    "\n",
    "            fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "            fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "            if words:\n",
    "                fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    "            if sentences:\n",
    "                fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "                fvs_punct[e, 1] = tokens.count(';') / float(len(sentences))\n",
    "                fvs_punct[e, 2] = tokens.count(':') / float(len(sentences))\n",
    "    \n",
    "        # apply whitening to decorrelate the features\n",
    "        fvs_lexical = whiten(fvs_lexical)\n",
    "        fvs_punct = whiten(fvs_punct)\n",
    "        feature_sets = list((np.nan_to_num(fvs_lexical), np.nan_to_num(fvs_punct)))\n",
    "        self.kmeans_cluster = self.predict_authors(feature_sets[0])\n",
    "        results = self.kmeans_cluster.labels_\n",
    "        self.sahil = 1 if sum(results) > len(results)/2 else 0\n",
    "        \n",
    "    def predict_authors(self, fvs):\n",
    "        km = KMeans(n_clusters=2, init='k-means++', n_init=10, verbose=0)\n",
    "        km.fit(fvs)\n",
    "        return km \n",
    "    \n",
    "    def predict(self, comment):\n",
    "        tokens = nltk.word_tokenize(comment.lower())\n",
    "        words = self.word_tokenizer.tokenize(comment.lower())\n",
    "        sentences = self.sentence_tokenizer.tokenize(comment)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(self.word_tokenizer.tokenize(s)) for s in sentences])\n",
    "        fvs_lexical = np.zeros((1, 3), np.float64)\n",
    "        fvs_lexical[0][0] = words_per_sentence.mean()\n",
    "        fvs_lexical[0][1] = words_per_sentence.std()\n",
    "        fvs_lexical[0][2] = len(vocab) / float(len(words)) if words else 0\n",
    "        return 1 if self.sahil == self.kmeans_cluster.predict(fvs_lexical)[0] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  1  score= 0.787037037037\n",
      "Test  2  score= 0.777777777778\n",
      "Test  3  score= 0.780092592593\n",
      "Test  4  score= 0.803240740741\n",
      "Test  5  score= 0.770833333333\n",
      "Test  6  score= 0.798611111111\n"
     ]
    }
   ],
   "source": [
    "test_times = 10\n",
    "total = 0.0\n",
    "for i in xrange(test_times):\n",
    "    cl = AuthorshipMasterClassifier('./data/vrckid.data', './data/')\n",
    "    cl.add_subclassifier(CharacterNGramsClassifier())\n",
    "    cl.add_subclassifier(BagOfWordsClassifier())\n",
    "    cl.add_subclassifier(PartOfSpeechClassifier(n=2))\n",
    "    cl.add_subclassifier(ShortMessageVerificationClassifier())\n",
    "    cl.add_subclassifier(LexicalKMeansClustering())\n",
    "    score = cl.test()\n",
    "    print 'Test ', i + 1, ' score=', score\n",
    "    total += score\n",
    "print 'Average score = ', total / test_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
